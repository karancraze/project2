{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "\n",
    "y_train = idx2numpy.convert_from_file('../dataset/train-labels.idx1-ubyte')\n",
    "y_test = idx2numpy.convert_from_file('../dataset/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in y_train:\n",
    "    text+=str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in y_test:\n",
    "    text+=str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "504192131\n"
     ]
    }
   ],
   "source": [
    "print (len(text))\n",
    "print (text[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 23317\n",
      "Unique characters: 10\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 50\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.LSTM(256, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "23317/23317 [==============================] - 11s 469us/step - loss: 2.4471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bdc37ca58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model for 1 epoch on the available training data\n",
    "model.fit(x, y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating with seed: \"75396859656403432447390061398132579991300662545093\"\n",
      "--- Original Text: \"94262685568701740492564289114513517448566317106576\"\n",
      "------ temperature: 0.2\n",
      "86666666666666666666668686666666666668666663666666\n",
      "------ temperature: 0.5\n",
      "66586668668856662628666662368868686366662858886626\n",
      "------ temperature: 1.0\n",
      "61636267166817688665936666866615383580663325885858\n",
      "------ temperature: 1.2\n",
      "50064768206666836656261666865262686777905662616832\n",
      "--- Generating with seed: \"27050571326633361620321891964140141341767179105162\"\n",
      "--- Original Text: \"73848566375809504122630437586940616233647516473819\"\n",
      "------ temperature: 0.2\n",
      "66666666666666866666686668666686668688686666686666\n",
      "------ temperature: 0.5\n",
      "56866818566666668586686686166888896665666665258666\n",
      "------ temperature: 1.0\n",
      "69488686573396586886861230662665636888688806962796\n",
      "------ temperature: 1.2\n",
      "68582816913846763863736626886292896622946885638888\n",
      "--- Generating with seed: \"61061911810646777061306804774640188431761496398407\"\n",
      "--- Original Text: \"17263140546177899309112031455180900713223646548891\"\n",
      "------ temperature: 0.2\n",
      "66666666666666666666666668666666666666666686666666\n",
      "------ temperature: 0.5\n",
      "58886878833668668668556656565686366666862636862666\n",
      "------ temperature: 1.0\n",
      "76670367882884362855668682386826663248629566663633\n",
      "------ temperature: 1.2\n",
      "29582836124683868193786689833526685826588776365566\n",
      "--- Generating with seed: \"17353883043776733293784954384857112028277729029324\"\n",
      "--- Original Text: \"46315154453530141867247922233604610224010504421074\"\n",
      "------ temperature: 0.2\n",
      "86866666666666666666628866666686686686666666668666\n",
      "------ temperature: 0.5\n",
      "66668666863666836068866866886676686668626566666658\n",
      "------ temperature: 1.0\n",
      "36165666836702667862581674288085877868038283861868\n",
      "------ temperature: 1.2\n",
      "25672666486835252713966840823866138219838260986665\n",
      "--- Generating with seed: \"21918453115731934655673845870265488216314905149720\"\n",
      "--- Original Text: \"39732458157728807709149067459529112136780929133651\"\n",
      "------ temperature: 0.2\n",
      "66666666666666666686666666666666666666666868666668\n",
      "------ temperature: 0.5\n",
      "86578668626666686836666665686886866866666666866666\n",
      "------ temperature: 1.0\n",
      "86898358536568748862336364601650658773325602362856\n",
      "------ temperature: 1.2\n",
      "46670964113686888606763875866946761686388689556128\n",
      "--- Generating with seed: \"56708090011023334155657984021924445160768984989607\"\n",
      "--- Original Text: \"18274571400634868121107034558050529724511311612911\"\n",
      "------ temperature: 0.2\n",
      "66666866866668686666666668666666686666666666666686\n",
      "------ temperature: 0.5\n",
      "66686666866888669888686666666662676586686668665666\n",
      "------ temperature: 1.0\n",
      "52883666108566826326829662398265862296684208744662\n",
      "------ temperature: 1.2\n",
      "62585366353695282886623266286231465367255868866686\n",
      "--- Generating with seed: \"54412893014472423020992307394758020330228535001226\"\n",
      "--- Original Text: \"39405260758591011825304956627183970110283343536075\"\n",
      "------ temperature: 0.2\n",
      "66668866666666668666666668666666666666666686668866\n",
      "------ temperature: 0.5\n",
      "68365866866668866888636626628688668668886158656263\n",
      "------ temperature: 1.0\n",
      "66537868568316836788438235865582880665888876836659\n",
      "------ temperature: 1.2\n",
      "52286666810666866837897529608317060426771566656236\n",
      "--- Generating with seed: \"39153668818250178705485289783721132475668073864807\"\n",
      "--- Original Text: \"20641207274544739472187145017224038020302313358790\"\n",
      "------ temperature: 0.2\n",
      "66866666668686666668666686666666868666666666666666\n",
      "------ temperature: 0.5\n",
      "68688668668768568866566667228283886668816866582088\n",
      "------ temperature: 1.0\n",
      "64753656560863675336656626656355865264481583558368\n",
      "------ temperature: 1.2\n",
      "36262436662830816760061617612681833887838638535888\n",
      "--- Generating with seed: \"22030837344757491893910598048266240471784775153169\"\n",
      "--- Original Text: \"71834673736242167929860565939328515716282377643750\"\n",
      "------ temperature: 0.2\n",
      "66666666666666866666666666686686666666666666666666\n",
      "------ temperature: 0.5\n",
      "76666666666666686866688966560661666266836888628616\n",
      "------ temperature: 1.0\n",
      "86508788961278575966867568662668886833388666886610\n",
      "------ temperature: 1.2\n",
      "66632026691668867687515668862672865558686255593686\n",
      "--- Generating with seed: \"85506779393013084468446148962599007060339721866402\"\n",
      "--- Original Text: \"06968064456421431886733959553953176692529070712253\"\n",
      "------ temperature: 0.2\n",
      "66666666686666866666666666866666666668666686666886\n",
      "------ temperature: 0.5\n",
      "66686388688668682866868866636868683686266385688888\n",
      "------ temperature: 1.0\n",
      "82613681678378688666458166667622626626036662266988\n",
      "------ temperature: 1.2\n",
      "69603866263556688666882877886262589168663666616666\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "chars_to_generate = 50\n",
    "test_to_perform = 10\n",
    "\n",
    "seed_text = []\n",
    "text_to_generate = []\n",
    "text_generated_0_2 = []\n",
    "text_generated_0_5 = []\n",
    "text_generated_1_0 = []\n",
    "text_generated_1_2 = []\n",
    "\n",
    "\n",
    "for epoch in range(test_to_perform):\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    end_index = start_index + maxlen\n",
    "    generated_text = text[start_index: end_index]\n",
    "    \n",
    "    seed_text.append(generated_text)\n",
    "    text_to_generate.append(text[end_index: end_index + chars_to_generate])\n",
    "\n",
    "    \n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "    print('--- Original Text: \"' + str(text_to_generate[epoch]) + '\"')\n",
    "    \n",
    "        \n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "\n",
    "        # We generate 50 characters\n",
    "        for i in range(chars_to_generate):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "            \n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "        if temperature == 0.2:\n",
    "            text_generated_0_2.append(generated_text)\n",
    "        if temperature == 0.5:\n",
    "            text_generated_0_5.append(generated_text)\n",
    "        if temperature == 1.0:\n",
    "            text_generated_1_0.append(generated_text)\n",
    "        if temperature == 1.2:\n",
    "            text_generated_1_2.append(generated_text)\n",
    "        \n",
    "        \n",
    "    #model.save('model_epoch_{}.hdf5'.format(epoch))\n",
    "    #model.save_weights('text_generator_gigantic_weights{}.h5'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed text:             75396859656403432447390061398132579991300662545093\n",
      "text to be generated:  94262685568701740492564289114513517448566317106576\n",
      "text generated 0.2:    86666666666666666666668686666666666668666663666666\n",
      "text generated 0.5:    66586668668856662628666662368868686366662858886626\n",
      "text generated 1.0:    61636267166817688665936666866615383580663325885858\n",
      "text generated 1.2:    50064768206666836656261666865262686777905662616832\n",
      "\n",
      "seed text:             27050571326633361620321891964140141341767179105162\n",
      "text to be generated:  73848566375809504122630437586940616233647516473819\n",
      "text generated 0.2:    66666666666666866666686668666686668688686666686666\n",
      "text generated 0.5:    56866818566666668586686686166888896665666665258666\n",
      "text generated 1.0:    69488686573396586886861230662665636888688806962796\n",
      "text generated 1.2:    68582816913846763863736626886292896622946885638888\n",
      "\n",
      "seed text:             61061911810646777061306804774640188431761496398407\n",
      "text to be generated:  17263140546177899309112031455180900713223646548891\n",
      "text generated 0.2:    66666666666666666666666668666666666666666686666666\n",
      "text generated 0.5:    58886878833668668668556656565686366666862636862666\n",
      "text generated 1.0:    76670367882884362855668682386826663248629566663633\n",
      "text generated 1.2:    29582836124683868193786689833526685826588776365566\n",
      "\n",
      "seed text:             17353883043776733293784954384857112028277729029324\n",
      "text to be generated:  46315154453530141867247922233604610224010504421074\n",
      "text generated 0.2:    86866666666666666666628866666686686686666666668666\n",
      "text generated 0.5:    66668666863666836068866866886676686668626566666658\n",
      "text generated 1.0:    36165666836702667862581674288085877868038283861868\n",
      "text generated 1.2:    25672666486835252713966840823866138219838260986665\n",
      "\n",
      "seed text:             21918453115731934655673845870265488216314905149720\n",
      "text to be generated:  39732458157728807709149067459529112136780929133651\n",
      "text generated 0.2:    66666666666666666686666666666666666666666868666668\n",
      "text generated 0.5:    86578668626666686836666665686886866866666666866666\n",
      "text generated 1.0:    86898358536568748862336364601650658773325602362856\n",
      "text generated 1.2:    46670964113686888606763875866946761686388689556128\n",
      "\n",
      "seed text:             56708090011023334155657984021924445160768984989607\n",
      "text to be generated:  18274571400634868121107034558050529724511311612911\n",
      "text generated 0.2:    66666866866668686666666668666666686666666666666686\n",
      "text generated 0.5:    66686666866888669888686666666662676586686668665666\n",
      "text generated 1.0:    52883666108566826326829662398265862296684208744662\n",
      "text generated 1.2:    62585366353695282886623266286231465367255868866686\n",
      "\n",
      "seed text:             54412893014472423020992307394758020330228535001226\n",
      "text to be generated:  39405260758591011825304956627183970110283343536075\n",
      "text generated 0.2:    66668866666666668666666668666666666666666686668866\n",
      "text generated 0.5:    68365866866668866888636626628688668668886158656263\n",
      "text generated 1.0:    66537868568316836788438235865582880665888876836659\n",
      "text generated 1.2:    52286666810666866837897529608317060426771566656236\n",
      "\n",
      "seed text:             39153668818250178705485289783721132475668073864807\n",
      "text to be generated:  20641207274544739472187145017224038020302313358790\n",
      "text generated 0.2:    66866666668686666668666686666666868666666666666666\n",
      "text generated 0.5:    68688668668768568866566667228283886668816866582088\n",
      "text generated 1.0:    64753656560863675336656626656355865264481583558368\n",
      "text generated 1.2:    36262436662830816760061617612681833887838638535888\n",
      "\n",
      "seed text:             22030837344757491893910598048266240471784775153169\n",
      "text to be generated:  71834673736242167929860565939328515716282377643750\n",
      "text generated 0.2:    66666666666666866666666666686686666666666666666666\n",
      "text generated 0.5:    76666666666666686866688966560661666266836888628616\n",
      "text generated 1.0:    86508788961278575966867568662668886833388666886610\n",
      "text generated 1.2:    66632026691668867687515668862672865558686255593686\n",
      "\n",
      "seed text:             85506779393013084468446148962599007060339721866402\n",
      "text to be generated:  06968064456421431886733959553953176692529070712253\n",
      "text generated 0.2:    66666666686666866666666666866666666668666686666886\n",
      "text generated 0.5:    66686388688668682866868866636868683686266385688888\n",
      "text generated 1.0:    82613681678378688666458166667622626626036662266988\n",
      "text generated 1.2:    69603866263556688666882877886262589168663666616666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(test_to_perform):\n",
    "    print ('seed text:            ', seed_text[x])\n",
    "    print ('text to be generated: ', text_to_generate[x])\n",
    "    print ('text generated 0.2:   ', text_generated_0_2[x])\n",
    "    print ('text generated 0.5:   ', text_generated_0_5[x])\n",
    "    print ('text generated 1.0:   ', text_generated_1_0[x])\n",
    "    print ('text generated 1.2:   ', text_generated_1_2[x])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
